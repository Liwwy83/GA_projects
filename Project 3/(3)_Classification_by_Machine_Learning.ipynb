{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2bd3a1a",
   "metadata": {},
   "source": [
    "# (3) Classification by Machine Learning\n",
    "3 algorithms, Naive-Bayes, Logistic Regression and Random Forest Classifier are used and compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a56919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out this cell if we want to see warnings\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca959767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "\n",
    "# set options for pandas DataFrame display\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9157e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "japanese = pd.read_csv('datasets/japanese.csv')\n",
    "korean = pd.read_csv('datasets/korean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b3d789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((995, 1), (976, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of rows\n",
    "japanese.shape, korean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e725b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-English characters\n",
    "japanese['post'] = japanese['post'].map(lambda x: re.sub(r\"[^\\x00-\\x7F]\", '', x))\n",
    "korean['post'] = korean['post'].map(lambda x: re.sub(r\"[^\\x00-\\x7F]\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d749af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y for modelling\n",
    "\n",
    "# combine japanese and korean 'post' feature into single pandas Series\n",
    "X = pd.concat([japanese['post'], korean['post']])\n",
    "\n",
    "# create labels 1 for Japanese and 0 for Korean\n",
    "y = pd.Series([1] * japanese.shape[0] + [0] * korean.shape[0], name='japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0732893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.50482\n",
       "0    0.49518\n",
       "Name: japanese, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proportion of japanese posts vs korean posts\n",
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033aad1",
   "metadata": {},
   "source": [
    "### Baseline Accuracy\n",
    "The baseline accuracy is 0.505 if all posts are predicted to be from the Japanese subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a2d0beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eng_jk', 'common_eng', 'common_eng_jk', 'eng_lem', 'eng_jk_lem',\n",
       "       'common_eng_lem', 'common_eng_jk_lem'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load stopwords lists\n",
    "stopwords = pd.read_csv('datasets/stopwords.csv', index_col='Unnamed: 0')\n",
    "# check indices\n",
    "stopwords.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e629a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to retrieve stopwords list\n",
    "def get_stopwords(index):\n",
    "    sw_list = stopwords.loc[index]['list'].replace(\"'\",\"\")[1:-1].split(\", \")\n",
    "    return [re.sub(r'\"', '', x) for x in sw_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5138a195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1478,), (493,), (1478,), (493,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "# check shape\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03db36",
   "metadata": {},
   "source": [
    "## Functions to Set and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a980660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pipe(algorithm='nb', vectorizer='cvec'):\n",
    "    \n",
    "    algo_dic = {\n",
    "        'nb': MultinomialNB(),\n",
    "        'logreg': LogisticRegression(solver='liblinear', random_state=42),\n",
    "        'forest': RandomForestClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    vect_dic = {\n",
    "        'cvec': CountVectorizer(token_pattern=r\"\\w+\"),\n",
    "        'tvec': TfidfVectorizer(token_pattern=r\"\\w+\")\n",
    "    }\n",
    "        \n",
    "    pipe = Pipeline([\n",
    "        (vectorizer, vect_dic[vectorizer]),\n",
    "        (algorithm, algo_dic[algorithm])\n",
    "    ])\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45ea5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(\n",
    "    algorithm='nb',\n",
    "    vectorizer='cvec',\n",
    "    max_df=[1.0], \n",
    "    max_features=[None], \n",
    "    min_df=[1], \n",
    "    ngram_range=[(1,1)],\n",
    "    preprocessor=[None], \n",
    "    stopwords=[None], \n",
    "    algo_params={}\n",
    "):\n",
    "    \n",
    "    params = {\n",
    "        vectorizer+'__max_df': max_df,\n",
    "        vectorizer+'__max_features': max_features,\n",
    "        vectorizer+'__min_df': min_df,\n",
    "        vectorizer+'__ngram_range': ngram_range,\n",
    "        vectorizer+'__preprocessor': preprocessor,\n",
    "        vectorizer+'__stop_words': stopwords,\n",
    "    }\n",
    "    params.update(algo_params)\n",
    "    \n",
    "    return GridSearchCV(set_pipe(algorithm, vectorizer), params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "054cdfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    # Fit model to training data.\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Score: {model.best_score_}\")\n",
    "    print(f\"Train Score: {model.score(X_train, y_train)}\")\n",
    "    print(f\"Test Score: {model.score(X_test, y_test)}\")\n",
    "    print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef0c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Multinomial Naive-Bayes and Logistic Regression\n",
    "def get_coef(model):\n",
    "    coef = model.best_estimator_.steps[1][1].coef_\n",
    "    features = model.best_estimator_.steps[0][1].get_feature_names_out()\n",
    "    df = pd.DataFrame(coef, columns=features, index=['coef'] ).T.sort_values('coef')\n",
    "    display(df.head(10))\n",
    "    display(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee7ec039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Random Forest Classifier\n",
    "def get_feature_impt(model):\n",
    "    importances = model.best_estimator_.steps[1][1].feature_importances_\n",
    "    features = model.best_estimator_.steps[0][1].get_feature_names_out()\n",
    "    df = pd.DataFrame(zip(features,importances)).sort_values(1, ascending=False)\n",
    "    display(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4bea61",
   "metadata": {},
   "source": [
    "## Untuned Models\n",
    "The first set of untuned models are run without removing the keywords 'japanese' and 'korean', while the second set are run after removing these 2 obvious keywords.\n",
    "### Keywords 'japanese' and 'korean' NOT Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29a8b5e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.84439761795694\n",
      "Train Score: 0.9296346414073072\n",
      "Test Score: 0.8823529411764706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       244\n",
      "           1       0.90      0.86      0.88       249\n",
      "\n",
      "    accuracy                           0.88       493\n",
      "   macro avg       0.88      0.88      0.88       493\n",
      "weighted avg       0.88      0.88      0.88       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_cvec = set_model('nb', 'cvec')\n",
    "eval_model(nb_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1408d82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8119285387081998\n",
      "Train Score: 0.9451962110960758\n",
      "Test Score: 0.8377281947261663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83       244\n",
      "           1       0.82      0.88      0.84       249\n",
      "\n",
      "    accuracy                           0.84       493\n",
      "   macro avg       0.84      0.84      0.84       493\n",
      "weighted avg       0.84      0.84      0.84       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_tvec = set_model('nb', 'tvec')\n",
    "eval_model(nb_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6a5c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8572721026110856\n",
      "Train Score: 0.9871447902571042\n",
      "Test Score: 0.8600405679513184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86       244\n",
      "           1       0.89      0.83      0.86       249\n",
      "\n",
      "    accuracy                           0.86       493\n",
      "   macro avg       0.86      0.86      0.86       493\n",
      "weighted avg       0.86      0.86      0.86       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_cvec = set_model('logreg', 'cvec')\n",
    "eval_model(logreg_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a3352e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8701053595968851\n",
      "Train Score: 0.9654939106901218\n",
      "Test Score: 0.8823529411764706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.89       244\n",
      "           1       0.93      0.83      0.88       249\n",
      "\n",
      "    accuracy                           0.88       493\n",
      "   macro avg       0.89      0.88      0.88       493\n",
      "weighted avg       0.89      0.88      0.88       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_tvec = set_model('logreg', 'tvec')\n",
    "eval_model(logreg_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e2c28be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8599610627576728\n",
      "Train Score: 0.9952638700947226\n",
      "Test Score: 0.8661257606490872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.88       244\n",
      "           1       0.94      0.79      0.86       249\n",
      "\n",
      "    accuracy                           0.87       493\n",
      "   macro avg       0.88      0.87      0.87       493\n",
      "weighted avg       0.88      0.87      0.87       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_cvec = set_model('forest', 'cvec')\n",
    "eval_model(forest_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "624156c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8572514887769124\n",
      "Train Score: 0.9952638700947226\n",
      "Test Score: 0.8356997971602435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.84       244\n",
      "           1       0.89      0.78      0.83       249\n",
      "\n",
      "    accuracy                           0.84       493\n",
      "   macro avg       0.84      0.84      0.84       493\n",
      "weighted avg       0.84      0.84      0.84       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_tvec = set_model('forest', 'tvec')\n",
    "eval_model(forest_tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e77bd8",
   "metadata": {},
   "source": [
    "### Results without removing keywords 'japanese' and 'korean'\n",
    "|Algorithm|Vectorizer|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|Multinomial Naive-Bayes|CountVectorizer|0.844|0.930|0.882|0.88|0.86|0.90|\n",
    "|Multinomial Naive-Bayes|TfidfVectorizer|0.812|0.945|0.838|0.84|0.88|0.80|\n",
    "|Logistic Regression|CountVectorizer|0.857|0.987|0.860|0.86|0.83|0.89|\n",
    "|Logistic Regression|TfidfVectorizer|0.870|0.965|0.882|0.88|0.83|0.93|\n",
    "|Random Forest Classifier|CountVectorizer|0.860|0.995|0.866|0.87|0.79|0.95|\n",
    "|Random Forest Classifier|TfidfVectorizer|0.857|0.995|0.836|0.84|0.78|0.90|\n",
    "\n",
    "The scores are all quite good with some overfit as can be seen from the higher Train Scores (above 0.9) compared to Test Scores (between 0.8 and 0.9). Random Forest Classifier greatly overfits the training data with Train Scores very close to 1.0.\n",
    "\n",
    "All models fare similarly in terms of the confusion matrix, with f1 Scores between 0.84 to 0.88. The rates of misclassifying Japanese as Korean vs misclassifying Korean as Japanese are about the same, though the Logistic Regression and Random Forest Classifier models have a lower sensitivity than specificity (misclassifies Japanese posts as Korean more frequently)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6492c3",
   "metadata": {},
   "source": [
    "### Keywords 'japanese' and 'korean' Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3de0a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7361245991754466\n",
      "Train Score: 0.8944519621109608\n",
      "Test Score: 0.7829614604462475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78       244\n",
      "           1       0.78      0.80      0.79       249\n",
      "\n",
      "    accuracy                           0.78       493\n",
      "   macro avg       0.78      0.78      0.78       493\n",
      "weighted avg       0.78      0.78      0.78       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_cvec_jk = set_model('nb', 'cvec', stopwords=[['japanese', 'korean']])\n",
    "eval_model(nb_cvec_jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f88e79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7165139715987173\n",
      "Train Score: 0.9133964817320703\n",
      "Test Score: 0.7505070993914807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.66      0.72       244\n",
      "           1       0.71      0.84      0.77       249\n",
      "\n",
      "    accuracy                           0.75       493\n",
      "   macro avg       0.76      0.75      0.75       493\n",
      "weighted avg       0.76      0.75      0.75       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_tvec_jk = set_model('nb', 'tvec', stopwords=[['japanese', 'korean']])\n",
    "eval_model(nb_tvec_jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0ec1b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7368048557031608\n",
      "Train Score: 0.979702300405954\n",
      "Test Score: 0.7363083164300203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.75       244\n",
      "           1       0.76      0.69      0.73       249\n",
      "\n",
      "    accuracy                           0.74       493\n",
      "   macro avg       0.74      0.74      0.74       493\n",
      "weighted avg       0.74      0.74      0.74       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_cvec_jk = set_model('logreg', 'cvec', stopwords=[['japanese', 'korean']])\n",
    "eval_model(logreg_cvec_jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88610122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7462620247366011\n",
      "Train Score: 0.922192151556157\n",
      "Test Score: 0.768762677484787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78       244\n",
      "           1       0.81      0.71      0.76       249\n",
      "\n",
      "    accuracy                           0.77       493\n",
      "   macro avg       0.77      0.77      0.77       493\n",
      "weighted avg       0.77      0.77      0.77       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_tvec_jk = set_model('logreg', 'tvec', stopwords=[['japanese', 'korean']])\n",
    "eval_model(logreg_tvec_jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24ba7cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7185364177737059\n",
      "Train Score: 0.9952638700947226\n",
      "Test Score: 0.7342799188640974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.81      0.75       244\n",
      "           1       0.78      0.66      0.71       249\n",
      "\n",
      "    accuracy                           0.73       493\n",
      "   macro avg       0.74      0.74      0.73       493\n",
      "weighted avg       0.74      0.73      0.73       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_cvec_jk = set_model('forest', 'cvec', stopwords=[['japanese', 'korean']])\n",
    "eval_model(forest_cvec_jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52bee875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.717888227210261\n",
      "Train Score: 0.9952638700947226\n",
      "Test Score: 0.7200811359026369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73       244\n",
      "           1       0.74      0.68      0.71       249\n",
      "\n",
      "    accuracy                           0.72       493\n",
      "   macro avg       0.72      0.72      0.72       493\n",
      "weighted avg       0.72      0.72      0.72       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_tvec_jk = set_model('forest', 'tvec', stopwords=[['japanese', 'korean']])\n",
    "eval_model(forest_tvec_jk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e758a",
   "metadata": {},
   "source": [
    "### Results after removing keywords 'japanese' and 'korean'\n",
    "|Algorithm|Vectorizer|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|Multinomial Naive-Bayes|CountVectorizer|0.736|0.894|0.783|0.78|0.80|0.77|\n",
    "|Multinomial Naive-Bayes|TfidfVectorizer|0.717|0.913|0.751|0.75|0.84|0.66|\n",
    "|Logistic Regression|CountVectorizer|0.737|0.980|0.736|0.74|0.69|0.78|\n",
    "|Logistic Regression|TfidfVectorizer|0.746|0.922|0.769|0.77|0.71|0.82|\n",
    "|Random Forest Classifier|CountVectorizer|0.719|0.995|0.734|0.73|0.66|0.81|\n",
    "|Random Forest Classifier|TfidfVectorizer|0.718|0.995|0.720|0.72|0.68|0.76|\n",
    "\n",
    "As expected, the scores have all dropped while overfit is even more pronounced with Test Scores dropping to below 0.8. Random Forest Classifier still greatly overfits the training data with Train Scores very close to 1.0.\n",
    "\n",
    "The f1 Scores have all dropped to below 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360a6c2",
   "metadata": {},
   "source": [
    "## Modelling with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47d7810d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7361245991754466\n",
      "Train Score: 0.8944519621109608\n",
      "Test Score: 0.7829614604462475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78       244\n",
      "           1       0.78      0.80      0.79       249\n",
      "\n",
      "    accuracy                           0.78       493\n",
      "   macro avg       0.78      0.78      0.78       493\n",
      "weighted avg       0.78      0.78      0.78       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_cvec_jk_lem = set_model('nb', 'cvec', \n",
    "                           stopwords=[['japanese', 'korean']], \n",
    "                           preprocessor=[WordNetLemmatizer().lemmatize])\n",
    "eval_model(nb_cvec_jk_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efb65bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7165139715987173\n",
      "Train Score: 0.9133964817320703\n",
      "Test Score: 0.7505070993914807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.66      0.72       244\n",
      "           1       0.71      0.84      0.77       249\n",
      "\n",
      "    accuracy                           0.75       493\n",
      "   macro avg       0.76      0.75      0.75       493\n",
      "weighted avg       0.76      0.75      0.75       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_tvec_jk_lem = set_model('nb', 'tvec', \n",
    "                           stopwords=[['japanese', 'korean']], \n",
    "                           preprocessor=[WordNetLemmatizer().lemmatize])\n",
    "eval_model(nb_tvec_jk_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad982272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7368048557031608\n",
      "Train Score: 0.979702300405954\n",
      "Test Score: 0.7363083164300203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.75       244\n",
      "           1       0.76      0.69      0.73       249\n",
      "\n",
      "    accuracy                           0.74       493\n",
      "   macro avg       0.74      0.74      0.74       493\n",
      "weighted avg       0.74      0.74      0.74       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_cvec_jk_lem = set_model('logreg', 'cvec', \n",
    "                               stopwords=[['japanese', 'korean']], \n",
    "                               preprocessor=[WordNetLemmatizer().lemmatize])\n",
    "eval_model(logreg_cvec_jk_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a900dec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7462620247366011\n",
      "Train Score: 0.922192151556157\n",
      "Test Score: 0.768762677484787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78       244\n",
      "           1       0.81      0.71      0.76       249\n",
      "\n",
      "    accuracy                           0.77       493\n",
      "   macro avg       0.77      0.77      0.77       493\n",
      "weighted avg       0.77      0.77      0.77       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_tvec_jk_lem = set_model('logreg', 'tvec', \n",
    "                               stopwords=[['japanese', 'korean']], \n",
    "                               preprocessor=[WordNetLemmatizer().lemmatize])\n",
    "eval_model(logreg_tvec_jk_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fba0cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7185364177737059\n",
      "Train Score: 0.9952638700947226\n",
      "Test Score: 0.7342799188640974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.81      0.75       244\n",
      "           1       0.78      0.66      0.71       249\n",
      "\n",
      "    accuracy                           0.73       493\n",
      "   macro avg       0.74      0.74      0.73       493\n",
      "weighted avg       0.74      0.73      0.73       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_cvec_jk_lem = set_model('forest', 'cvec', \n",
    "                               stopwords=[['japanese', 'korean']], \n",
    "                               preprocessor=[WordNetLemmatizer().lemmatize])\n",
    "eval_model(forest_cvec_jk_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a37dda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.717888227210261\n",
      "Train Score: 0.9952638700947226\n",
      "Test Score: 0.7200811359026369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73       244\n",
      "           1       0.74      0.68      0.71       249\n",
      "\n",
      "    accuracy                           0.72       493\n",
      "   macro avg       0.72      0.72      0.72       493\n",
      "weighted avg       0.72      0.72      0.72       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_tvec_jk_lem = set_model('forest', 'tvec', \n",
    "                               stopwords=[['japanese', 'korean']], \n",
    "                               preprocessor=[WordNetLemmatizer().lemmatize])\n",
    "eval_model(forest_tvec_jk_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba23cd",
   "metadata": {},
   "source": [
    "### Results with Lemmatization\n",
    "The results are the same for all models. This is consistent with the analysis from part (2) Data Analysis, where it was found that there was very little difference between the lemmatized and non-lemmatized sets of common words in the 2 subreddits. The models are tuned without Lemmatization using GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da007e23",
   "metadata": {},
   "source": [
    "## Tuning Models without Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af586ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7598190563444801\n",
      "Train Score: 0.8930987821380244\n",
      "Test Score: 0.7890466531440162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78       244\n",
      "           1       0.78      0.81      0.80       249\n",
      "\n",
      "    accuracy                           0.79       493\n",
      "   macro avg       0.79      0.79      0.79       493\n",
      "weighted avg       0.79      0.79      0.79       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first attempt at tuning nb cvec model\n",
    "nb_cvec_tuned = set_model('nb', 'cvec', \n",
    "                          max_df=[.2,.8],\n",
    "                          max_features=[None, 3000, 5000],\n",
    "                          min_df=[1,2],\n",
    "                          ngram_range=[(1,1),(1,2)],\n",
    "                          stopwords=[['japanese', 'korean'], \n",
    "                                     get_stopwords('eng_jk'), \n",
    "                                     get_stopwords('common_eng_jk')], \n",
    "                          algo_params={\n",
    "                              'nb__alpha': [1.0e-10,.1,1.0]\n",
    "                          })\n",
    "eval_model(nb_cvec_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2160bb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.2,\n",
       " 'cvec__max_features': 5000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__preprocessor': None,\n",
       " 'cvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean'],\n",
       " 'nb__alpha': 1.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cvec_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083fcad",
   "metadata": {},
   "source": [
    "### Results\n",
    "|Tuning|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Before|0.736|0.894|0.783|0.78|0.80|0.77|\n",
    "|1st Tuning|0.760|0.893|0.789|0.79|0.81|0.77|\n",
    "\n",
    "There is a slight improvement in scores and the overfit is reduced.\n",
    "\n",
    "The best parameters of min_df=1, stopwords=common_eng_jk are retained and the rest undergo a second set of tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51169e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7625217590471827\n",
      "Train Score: 0.8944519621109608\n",
      "Test Score: 0.7931034482758621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.79       244\n",
      "           1       0.78      0.82      0.80       249\n",
      "\n",
      "    accuracy                           0.79       493\n",
      "   macro avg       0.79      0.79      0.79       493\n",
      "weighted avg       0.79      0.79      0.79       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second attempt at tuning nb cvec model\n",
    "nb_cvec_tuned_2 = set_model('nb', 'cvec', \n",
    "                            max_df=[.1,.2,.4],\n",
    "                            max_features=[4000,5000,6000],\n",
    "                            ngram_range=[(1,2),(1,3),(2,3)],\n",
    "                            stopwords=[get_stopwords('common_eng_jk')], \n",
    "                            algo_params={\n",
    "                                'nb__alpha': [.8,1.0]\n",
    "                            })\n",
    "eval_model(nb_cvec_tuned_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38b97e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.2,\n",
       " 'cvec__max_features': 5000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 3),\n",
       " 'cvec__preprocessor': None,\n",
       " 'cvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean'],\n",
       " 'nb__alpha': 0.8}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cvec_tuned_2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38f5f0",
   "metadata": {},
   "source": [
    "### Results\n",
    "|Tuning|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Before|0.736|0.894|0.783|0.78|0.80|0.77|\n",
    "|1st Tuning|0.760|0.893|0.789|0.79|0.81|0.77|\n",
    "|2nd Tuning|0.763|0.894|0.793|0.79|0.82|0.77|\n",
    "\n",
    "There is only a slight decrease in scores and reduction in overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93f2797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7604947320201557\n",
      "Train Score: 0.959404600811908\n",
      "Test Score: 0.7789046653144016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.74      0.77       244\n",
      "           1       0.76      0.82      0.79       249\n",
      "\n",
      "    accuracy                           0.78       493\n",
      "   macro avg       0.78      0.78      0.78       493\n",
      "weighted avg       0.78      0.78      0.78       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first attempt at tuning nb tvec model\n",
    "nb_tvec_tuned = set_model('nb', 'tvec', \n",
    "                          max_df=[.2,.8],\n",
    "                          max_features=[None, 3000, 5000],\n",
    "                          min_df=[1,2],\n",
    "                          ngram_range=[(1,1),(1,2)],\n",
    "                          stopwords=[['japanese', 'korean'], \n",
    "                                     get_stopwords('eng_jk'), \n",
    "                                     get_stopwords('common_eng_jk')], \n",
    "                          algo_params={\n",
    "                              'nb__alpha': [1.0e-10,.1,1.0]\n",
    "                          })\n",
    "eval_model(nb_tvec_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "729b20a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.1,\n",
       " 'tvec__max_df': 0.2,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_tvec_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b0096",
   "metadata": {},
   "source": [
    "|Tuning|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Before|0.717|0.913|0.751|0.75|0.84|0.66|\n",
    "|1st Tuning|0.760|0.959|0.779|0.78|0.82|0.74|\n",
    "\n",
    "There are some improvement in scores, but the overfit also became more pronounced. The f1 score improved as the sensitivity dropped but the specificity improved more. \n",
    "\n",
    "The best parameters max_features=None, stopwords=common_eng_jk are retained and the rest undergo a second set of tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b36808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7611704076958314\n",
      "Train Score: 0.945872801082544\n",
      "Test Score: 0.7849898580121704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.78       244\n",
      "           1       0.77      0.82      0.79       249\n",
      "\n",
      "    accuracy                           0.78       493\n",
      "   macro avg       0.79      0.78      0.78       493\n",
      "weighted avg       0.79      0.78      0.78       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second attempt at tuning nb tvec model\n",
    "nb_tvec_tuned_2 = set_model('nb', 'tvec', \n",
    "                            max_df=[.1,.2,.4],\n",
    "                            min_df=[2,3,4],\n",
    "                            ngram_range=[(1,2), (1,3), (2,3)],\n",
    "                            stopwords=[get_stopwords('common_eng_jk')], \n",
    "                            algo_params={\n",
    "                                'nb__alpha': [.1,.2,.4]\n",
    "                            })\n",
    "eval_model(nb_tvec_tuned_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "063ca664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.4,\n",
       " 'tvec__max_df': 0.2,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_tvec_tuned_2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6793b",
   "metadata": {},
   "source": [
    "|Tuning|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Before|0.717|0.913|0.751|0.75|0.84|0.66|\n",
    "|1st Tuning|0.760|0.959|0.779|0.78|0.82|0.74|\n",
    "|2nd Tuning|0.761|0.946|0.785|0.78|0.82|0.75|\n",
    "\n",
    "There is a slight improvement in scores and the overfit is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "599c32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7604901511681172\n",
      "Train Score: 0.969553450608931\n",
      "Test Score: 0.7626774847870182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.78       244\n",
      "           1       0.81      0.70      0.75       249\n",
      "\n",
      "    accuracy                           0.76       493\n",
      "   macro avg       0.77      0.76      0.76       493\n",
      "weighted avg       0.77      0.76      0.76       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first attempt at tuning logreg cvec model\n",
    "logreg_cvec_tuned = set_model('logreg', 'cvec', \n",
    "                              max_df=[.2,.8],\n",
    "                              max_features=[None, 3000, 5000],\n",
    "                              min_df=[1,2],\n",
    "                              ngram_range=[(1,1),(1,2)],\n",
    "                              stopwords=[['japanese', 'korean'], \n",
    "                                         get_stopwords('eng_jk'), \n",
    "                                         get_stopwords('common_eng_jk')], \n",
    "                              algo_params={\n",
    "                                  'logreg__C': [.2,.8],\n",
    "                                  'logreg__max_iter': [100, 200],\n",
    "                                  'logreg__penalty': ['l1', 'l2']\n",
    "                              })\n",
    "eval_model(logreg_cvec_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9564eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.2,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__preprocessor': None,\n",
       " 'cvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean'],\n",
       " 'logreg__C': 0.8,\n",
       " 'logreg__max_iter': 100,\n",
       " 'logreg__penalty': 'l2'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check best parameters\n",
    "logreg_cvec_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65501733",
   "metadata": {},
   "source": [
    "### Results\n",
    "|Tuning|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Before|0.737|0.980|0.736|0.74|0.69|0.78|\n",
    "|1st Tuning|0.760|0.970|0.763|0.76|0.70|0.83|\n",
    "\n",
    "There is an improvement in scores and the overfit is reduced.\n",
    "\n",
    "The best parameters of max_features=None, stopwords=common_eng_jk, max_iter=100, penalty='l2' are retained and the rest undergo a second set of tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48965576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7537379752633991\n",
      "Train Score: 0.959404600811908\n",
      "Test Score: 0.7545638945233266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.77       244\n",
      "           1       0.79      0.69      0.74       249\n",
      "\n",
      "    accuracy                           0.75       493\n",
      "   macro avg       0.76      0.76      0.75       493\n",
      "weighted avg       0.76      0.75      0.75       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second attempt at tuning logreg cvec model\n",
    "logreg_cvec_tuned_2 = set_model('logreg', 'cvec', \n",
    "                                max_df=[.1,.2,.4],\n",
    "                                min_df=[2,3,4],\n",
    "                                stopwords=[get_stopwords('common_eng_jk')], \n",
    "                                algo_params={\n",
    "                                    'logreg__C': [.6,.8,1.0]\n",
    "                                })\n",
    "eval_model(logreg_cvec_tuned_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dbea4c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.2,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__preprocessor': None,\n",
       " 'cvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean'],\n",
       " 'logreg__C': 0.6}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check best parameters\n",
    "logreg_cvec_tuned_2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886bfe4d",
   "metadata": {},
   "source": [
    "### Results\n",
    "|Tuning|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Before|0.737|0.980|0.736|0.74|0.69|0.78|\n",
    "|1st Tuning|0.760|0.970|0.763|0.76|0.70|0.83|\n",
    "|2nd Tuning|0.754|0.959|0.755|0.75|0.69|0.82|\n",
    "\n",
    "The scores dropped slightly. The model from the first tuning is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "388338da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7645327530920751\n",
      "Train Score: 0.9201623815967523\n",
      "Test Score: 0.7606490872210954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77       244\n",
      "           1       0.79      0.71      0.75       249\n",
      "\n",
      "    accuracy                           0.76       493\n",
      "   macro avg       0.76      0.76      0.76       493\n",
      "weighted avg       0.76      0.76      0.76       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first attempt at tuning logreg tvec model\n",
    "logreg_tvec_tuned = set_model('logreg', 'tvec', \n",
    "                              max_df=[.2,.8],\n",
    "                              max_features=[None, 3000, 5000],\n",
    "                              min_df=[1,2],\n",
    "                              ngram_range=[(1,1),(1,2)],\n",
    "                              stopwords=[['japanese', 'korean'], \n",
    "                                         get_stopwords('eng_jk'), \n",
    "                                         get_stopwords('common_eng_jk')], \n",
    "                              algo_params={\n",
    "                                  'logreg__C': [.2,.8],\n",
    "                                  'logreg__max_iter': [100, 200],\n",
    "                                  'logreg__penalty': ['l1','l2']\n",
    "                              })\n",
    "eval_model(logreg_tvec_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed837bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg__C': 0.8,\n",
       " 'logreg__max_iter': 100,\n",
       " 'logreg__penalty': 'l2',\n",
       " 'tvec__max_df': 0.2,\n",
       " 'tvec__max_features': 5000,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__stop_words': ['japanese', 'korean']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_tvec_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18574b",
   "metadata": {},
   "source": [
    "### Results\n",
    "|Tuning|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Before|0.746|0.922|0.769|0.77|0.71|0.82|\n",
    "|1st Tuning|0.765|0.920|0.761|0.76|0.71|0.81|\n",
    "\n",
    "The Best Score is higher but all other scores got worse, including more overfitting. The tuning of this model is abandoned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b168fb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "960 fits failed out of a total of 5760.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "960 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1344, in fit_transform\n",
      "    raise ValueError(\"max_df corresponds to < documents than min_df\")\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.73884792 0.73680715 0.74425103 ...        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7598007329363261\n",
      "Train Score: 0.9851150202976996\n",
      "Test Score: 0.7525354969574036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.76       244\n",
      "           1       0.77      0.73      0.75       249\n",
      "\n",
      "    accuracy                           0.75       493\n",
      "   macro avg       0.75      0.75      0.75       493\n",
      "weighted avg       0.75      0.75      0.75       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first attempt at tuning forest cvec model\n",
    "forest_cvec_tuned = set_model('forest', 'cvec', \n",
    "                              max_df=[.6,.8,1],\n",
    "                              max_features=[None,3000],\n",
    "                              min_df=[1,2],\n",
    "                              ngram_range=[(1,1),(1,2)],\n",
    "                              stopwords=[['japanese', 'korean'], \n",
    "                                         get_stopwords('common_eng_jk')], \n",
    "                              algo_params={\n",
    "                                  'forest__n_estimators': [100,200],\n",
    "                                  'forest__max_depth': [None,5,10],\n",
    "                                  'forest__min_samples_split': [2,3],\n",
    "                                  'forest__ccp_alpha': [0,.1],\n",
    "                                  'forest__n_jobs': [-1],\n",
    "#                                   'forest__verbose': [1]\n",
    "                              })\n",
    "eval_model(forest_cvec_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50bd8f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.6,\n",
       " 'cvec__max_features': 3000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__preprocessor': None,\n",
       " 'cvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean'],\n",
       " 'forest__ccp_alpha': 0,\n",
       " 'forest__max_depth': None,\n",
       " 'forest__min_samples_split': 2,\n",
       " 'forest__n_estimators': 200,\n",
       " 'forest__n_jobs': -1}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_cvec_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248ccf5",
   "metadata": {},
   "source": [
    "### Results\n",
    "|Tuning|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Before|0.719|0.995|0.734|0.73|0.66|0.81|\n",
    "|1st Tuning|0.760|0.985|0.753|0.75|0.73|0.77|\n",
    "\n",
    "The scores improved and the overfit has been reduced.\n",
    "\n",
    "The best parameters of min_df=1, stopwords=common_eng_jk, ccp_alpha=0, max_depth=None, min_samples_split=2 are retained and the rest undergo a second set of tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "429e8c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7598007329363261\n",
      "Train Score: 0.9851150202976996\n",
      "Test Score: 0.7525354969574036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.76       244\n",
      "           1       0.77      0.73      0.75       249\n",
      "\n",
      "    accuracy                           0.75       493\n",
      "   macro avg       0.75      0.75      0.75       493\n",
      "weighted avg       0.75      0.75      0.75       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second attempt at tuning forest cvec model\n",
    "forest_cvec_tuned_2 = set_model('forest', 'cvec', \n",
    "                                max_df=[.2,.4,.6],\n",
    "                                max_features=[3000,5000],\n",
    "                                ngram_range=[(1,2),(1,3),(2,3)],\n",
    "                                stopwords=[get_stopwords('common_eng_jk')], \n",
    "                                algo_params={\n",
    "                                    'forest__n_estimators': [200,300],\n",
    "                                    'forest__n_jobs': [-1],\n",
    "#                                     'forest__verbose': [1]\n",
    "                                })\n",
    "eval_model(forest_cvec_tuned_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d8b4354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.2,\n",
       " 'cvec__max_features': 3000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__preprocessor': None,\n",
       " 'cvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean'],\n",
       " 'forest__n_estimators': 200,\n",
       " 'forest__n_jobs': -1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_cvec_tuned_2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f984a658",
   "metadata": {},
   "source": [
    "### Results\n",
    "The parameters chosen did not change, so the results are identical to the previous set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "294372af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "960 fits failed out of a total of 5760.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "960 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2077, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1344, in fit_transform\n",
      "    raise ValueError(\"max_df corresponds to < documents than min_df\")\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.70768209 0.76048328 0.6934929  ...        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7645442052221714\n",
      "Train Score: 0.993234100135318\n",
      "Test Score: 0.7261663286004056\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74       244\n",
      "           1       0.76      0.67      0.71       249\n",
      "\n",
      "    accuracy                           0.73       493\n",
      "   macro avg       0.73      0.73      0.73       493\n",
      "weighted avg       0.73      0.73      0.73       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first attempt at tuning forest tvec model\n",
    "forest_tvec_tuned = set_model('forest', 'tvec', \n",
    "                              max_df=[.6,.8,1],\n",
    "                              max_features=[None,5000],\n",
    "                              min_df=[1,2],\n",
    "                              ngram_range=[(1,1),(1,2)],\n",
    "                              stopwords=[['japanese', 'korean'], \n",
    "                                         get_stopwords('common_eng_jk')], \n",
    "                              algo_params={\n",
    "                                  'forest__n_estimators': [100,200],\n",
    "                                  'forest__max_depth': [None,5,10],\n",
    "                                  'forest__min_samples_split': [2,3],\n",
    "                                  'forest__ccp_alpha': [0,.1],\n",
    "                                  'forest__n_jobs': [-1],\n",
    "#                                   'forest__verbose': [1]\n",
    "                              })\n",
    "eval_model(forest_tvec_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b3c76a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest__ccp_alpha': 0,\n",
       " 'forest__max_depth': None,\n",
       " 'forest__min_samples_split': 3,\n",
       " 'forest__n_estimators': 100,\n",
       " 'forest__n_jobs': -1,\n",
       " 'tvec__max_df': 0.6,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_tvec_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f2983",
   "metadata": {},
   "source": [
    "### Results\n",
    "|Tuning|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Before|0.718|0.995|0.720|0.72|0.68|0.76|\n",
    "|1st Tuning|0.765|0.993|0.726|0.73|0.67|0.79|\n",
    "\n",
    "The scores improved and the overfit has been slightly reduced.\n",
    "\n",
    "The best parameters of max_features=None, min_df=1, ngram_range=(1,1), stopwords=common_eng_jk, ccp_alpha=0, max_depth=None, n_estimators=100 are retained and the rest undergo a second set of tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7476a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7645442052221714\n",
      "Train Score: 0.993234100135318\n",
      "Test Score: 0.7261663286004056\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74       244\n",
      "           1       0.76      0.67      0.71       249\n",
      "\n",
      "    accuracy                           0.73       493\n",
      "   macro avg       0.73      0.73      0.73       493\n",
      "weighted avg       0.73      0.73      0.73       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second attempt at tuning forest tvec model\n",
    "forest_tvec_tuned_2 = set_model('forest', 'tvec', \n",
    "                                max_df=[.2,.4,.6],\n",
    "                                stopwords=[get_stopwords('common_eng_jk')], \n",
    "                                algo_params={\n",
    "                                    'forest__min_samples_split': [3,5,7],\n",
    "                                    'forest__n_jobs': [-1],\n",
    "#                                     'forest__verbose': [1]\n",
    "                                })\n",
    "eval_model(forest_tvec_tuned_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3fedc4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest__min_samples_split': 3,\n",
       " 'forest__n_jobs': -1,\n",
       " 'tvec__max_df': 0.2,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__stop_words': ['word',\n",
       "  'understand',\n",
       "  'good',\n",
       "  'also',\n",
       "  'lot',\n",
       "  'used',\n",
       "  'learning',\n",
       "  'much',\n",
       "  'something',\n",
       "  'english',\n",
       "  'anyone',\n",
       "  'first',\n",
       "  'ive',\n",
       "  'learn',\n",
       "  'know',\n",
       "  'way',\n",
       "  'words',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  'people',\n",
       "  'help',\n",
       "  'dont',\n",
       "  'get',\n",
       "  'grammar',\n",
       "  'really',\n",
       "  'read',\n",
       "  'find',\n",
       "  'time',\n",
       "  'want',\n",
       "  'use',\n",
       "  'language',\n",
       "  'would',\n",
       "  'think',\n",
       "  'im',\n",
       "  'study',\n",
       "  'one',\n",
       "  'studying',\n",
       "  'i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'youll',\n",
       "  'youd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'shes',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'thatll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'dont',\n",
       "  'should',\n",
       "  'shouldve',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'arent',\n",
       "  'couldn',\n",
       "  'couldnt',\n",
       "  'didn',\n",
       "  'didnt',\n",
       "  'doesn',\n",
       "  'doesnt',\n",
       "  'hadn',\n",
       "  'hadnt',\n",
       "  'hasn',\n",
       "  'hasnt',\n",
       "  'haven',\n",
       "  'havent',\n",
       "  'isn',\n",
       "  'isnt',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightnt',\n",
       "  'mustn',\n",
       "  'mustnt',\n",
       "  'needn',\n",
       "  'neednt',\n",
       "  'shan',\n",
       "  'shant',\n",
       "  'shouldn',\n",
       "  'shouldnt',\n",
       "  'wasn',\n",
       "  'wasnt',\n",
       "  'weren',\n",
       "  'werent',\n",
       "  'won',\n",
       "  'wont',\n",
       "  'wouldn',\n",
       "  'wouldnt',\n",
       "  'japanese',\n",
       "  'korean']}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_tvec_tuned_2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedccc1",
   "metadata": {},
   "source": [
    "### Results\n",
    "The best parameter for max_df changed from 0.6 to 0.2 but the scores remained the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e14463",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "|Algorithm|Vectorizer|Best Score|Train Score|Test Score|f1 Score|Sensitivity|Specificity|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|Multinomial Naive-Bayes|CountVectorizer|0.763|0.894|0.793|0.79|0.82|0.77|\n",
    "|Multinomial Naive-Bayes|TfidfVectorizer|0.761|0.946|0.785|0.78|0.82|0.75|\n",
    "|Logistic Regression|CountVectorizer|0.760|0.970|0.763|0.76|0.70|0.83|\n",
    "|Logistic Regression|TfidfVectorizer|0.746|0.922|0.769|0.77|0.71|0.82|\n",
    "|Random Forest Classifier|CountVectorizer|0.760|0.985|0.753|0.75|0.73|0.77|\n",
    "|Random Forest Classifier|CountVectorizer|0.765|0.993|0.726|0.73|0.67|0.79|\n",
    "\n",
    "The scores are quite similar for all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0893854",
   "metadata": {},
   "source": [
    "## Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e3f4e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sound natural</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel comfortable</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel better someone</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel better</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speak speak</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speak speak speak</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feed</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker appreciate</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker appreciate idk</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker saying</th>\n",
       "      <td>-10.599038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             coef\n",
       "sound natural          -10.599038\n",
       "feel comfortable       -10.599038\n",
       "feel better someone    -10.599038\n",
       "feel better            -10.599038\n",
       "speak speak            -10.599038\n",
       "speak speak speak      -10.599038\n",
       "feed                   -10.599038\n",
       "speaker appreciate     -10.599038\n",
       "speaker appreciate idk -10.599038\n",
       "speaker saying         -10.599038"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>japan</th>\n",
       "      <td>-5.752884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking</th>\n",
       "      <td>-5.743109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>-5.668168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make</th>\n",
       "      <td>-5.650278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anki</th>\n",
       "      <td>-5.632703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>using</th>\n",
       "      <td>-5.573514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>-5.565338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reading</th>\n",
       "      <td>-5.442860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genki</th>\n",
       "      <td>-5.340241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kanji</th>\n",
       "      <td>-4.428382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef\n",
       "japan   -5.752884\n",
       "looking -5.743109\n",
       "start   -5.668168\n",
       "make    -5.650278\n",
       "anki    -5.632703\n",
       "using   -5.573514\n",
       "n       -5.565338\n",
       "reading -5.442860\n",
       "genki   -5.340241\n",
       "kanji   -4.428382"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for Multinomial Naive-Bayes CountVectorizer\n",
    "get_coef(nb_cvec_tuned_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dee4699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/family/opt/anaconda3/envs/nov20/lib/python3.8/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>listening reading</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone talk</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone says</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developer</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developers</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone pronounce</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone name</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dict</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone journey</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone interested</th>\n",
       "      <td>-9.543967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        coef\n",
       "listening reading  -9.543967\n",
       "someone talk       -9.543967\n",
       "someone says       -9.543967\n",
       "developer          -9.543967\n",
       "developers         -9.543967\n",
       "someone pronounce  -9.543967\n",
       "someone name       -9.543967\n",
       "dict               -9.543967\n",
       "someone journey    -9.543967\n",
       "someone interested -9.543967"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>app</th>\n",
       "      <td>-6.409040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>katakana</th>\n",
       "      <td>-6.336903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>-6.312437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reading</th>\n",
       "      <td>-6.297708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hiragana</th>\n",
       "      <td>-6.281219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anki</th>\n",
       "      <td>-6.248949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>-6.141950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>-6.132406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genki</th>\n",
       "      <td>-6.002521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kanji</th>\n",
       "      <td>-5.180831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              coef\n",
       "app      -6.409040\n",
       "katakana -6.336903\n",
       "question -6.312437\n",
       "reading  -6.297708\n",
       "hiragana -6.281219\n",
       "anki     -6.248949\n",
       "start    -6.141950\n",
       "n        -6.132406\n",
       "genki    -6.002521\n",
       "kanji    -5.180831"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for Multinomial Naive-Bayes TfidfVectorizer\n",
    "get_coef(nb_tvec_tuned_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433510b0",
   "metadata": {},
   "source": [
    "For Multinomial Naive-Bayes, only the important features for classifying a post as Japanese can be picked out from the words with less negative coefficients. For example, in the TfidfVectorizer model, the writing systems 'katakana', 'hiragana' and 'kanji', the commonly used flashcard app 'anki' and the commonly used textbook 'genki'. The other words like 'app', 'question', 'reading', 'start' and 'n' have no specific link to the Japanese language and are just as likely to appear in the Korean subreddit. It is likely that the model overtrained on these features. The top ten features picked up by the CountVectorizer model are even less specific to Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9ee1187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>korea</th>\n",
       "      <td>-1.634645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttmik</th>\n",
       "      <td>-1.262089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hangul</th>\n",
       "      <td>-1.116330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hanja</th>\n",
       "      <td>-0.852709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interchangeable</th>\n",
       "      <td>-0.815994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intermediate</th>\n",
       "      <td>-0.777432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friends</th>\n",
       "      <td>-0.767315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topik</th>\n",
       "      <td>-0.748029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <td>-0.738193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kpop</th>\n",
       "      <td>-0.731950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     coef\n",
       "korea           -1.634645\n",
       "ttmik           -1.262089\n",
       "hangul          -1.116330\n",
       "hanja           -0.852709\n",
       "interchangeable -0.815994\n",
       "intermediate    -0.777432\n",
       "friends         -0.767315\n",
       "topik           -0.748029\n",
       "exchange        -0.738193\n",
       "kpop            -0.731950"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nihongo</th>\n",
       "      <td>0.963521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app</th>\n",
       "      <td>0.992204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jlpt</th>\n",
       "      <td>1.035425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>katakana</th>\n",
       "      <td>1.073910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anime</th>\n",
       "      <td>1.179843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hiragana</th>\n",
       "      <td>1.239954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>1.272942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genki</th>\n",
       "      <td>1.674153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japan</th>\n",
       "      <td>1.698325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kanji</th>\n",
       "      <td>2.385682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              coef\n",
       "nihongo   0.963521\n",
       "app       0.992204\n",
       "jlpt      1.035425\n",
       "katakana  1.073910\n",
       "anime     1.179843\n",
       "hiragana  1.239954\n",
       "n         1.272942\n",
       "genki     1.674153\n",
       "japan     1.698325\n",
       "kanji     2.385682"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_coef(logreg_cvec_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb3e4e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>korea</th>\n",
       "      <td>-1.935336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttmik</th>\n",
       "      <td>-1.715643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>-1.373717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>-1.272829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hanja</th>\n",
       "      <td>-1.155859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hangul</th>\n",
       "      <td>-1.153130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topik</th>\n",
       "      <td>-1.095048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intermediate</th>\n",
       "      <td>-1.090534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone</th>\n",
       "      <td>-1.055295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk</th>\n",
       "      <td>-1.046582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef\n",
       "korea        -1.935336\n",
       "ttmik        -1.715643\n",
       "name         -1.373717\n",
       "you          -1.272829\n",
       "hanja        -1.155859\n",
       "hangul       -1.153130\n",
       "topik        -1.095048\n",
       "intermediate -1.090534\n",
       "someone      -1.055295\n",
       "talk         -1.046582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>through</th>\n",
       "      <td>1.351978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anime</th>\n",
       "      <td>1.377047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anki</th>\n",
       "      <td>1.484723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app</th>\n",
       "      <td>1.497710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>katakana</th>\n",
       "      <td>1.864233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hiragana</th>\n",
       "      <td>1.933414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japan</th>\n",
       "      <td>2.039729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>2.367527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genki</th>\n",
       "      <td>2.477423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kanji</th>\n",
       "      <td>4.604094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              coef\n",
       "through   1.351978\n",
       "anime     1.377047\n",
       "anki      1.484723\n",
       "app       1.497710\n",
       "katakana  1.864233\n",
       "hiragana  1.933414\n",
       "japan     2.039729\n",
       "n         2.367527\n",
       "genki     2.477423\n",
       "kanji     4.604094"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_coef(logreg_tvec_jk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ca2ce",
   "metadata": {},
   "source": [
    "For Logistic Regression, the more negative coefficients are words linked to Korean while the more positive coefficients are words linked to Japanese. For example, there are words related to the writing systems ('hangul' and 'hanja' for Korean and 'katakana', 'hiragana' and 'kanji' for Japanese), words related to the popular cultures ('kpop' for Korean and 'anime' for Japanese) and the proficiency tests ('topik' for Korean and 'jlpt' for Japanese). The words used for classifying which subreddit the posts belong to appear to be more correctly picked out, giving more confidence in the predictive powers of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55ee2680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>kanji</td>\n",
       "      <td>0.047046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>genki</td>\n",
       "      <td>0.018677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>japan</td>\n",
       "      <td>0.014082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>hiragana</td>\n",
       "      <td>0.012954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>n</td>\n",
       "      <td>0.012795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>korea</td>\n",
       "      <td>0.011741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>katakana</td>\n",
       "      <td>0.009625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>anime</td>\n",
       "      <td>0.009507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>anki</td>\n",
       "      <td>0.008314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>start</td>\n",
       "      <td>0.007788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>ttmik</td>\n",
       "      <td>0.007702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>jlpt</td>\n",
       "      <td>0.006983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>app</td>\n",
       "      <td>0.006177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>wanikani</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>say</td>\n",
       "      <td>0.004388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>name</td>\n",
       "      <td>0.004383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>meaning</td>\n",
       "      <td>0.003735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>game</td>\n",
       "      <td>0.003598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>manga</td>\n",
       "      <td>0.003596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>thank</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "1325     kanji  0.047046\n",
       "985      genki  0.018677\n",
       "1287     japan  0.014082\n",
       "1135  hiragana  0.012954\n",
       "1685         n  0.012795\n",
       "1391     korea  0.011741\n",
       "1345  katakana  0.009625\n",
       "108      anime  0.009507\n",
       "109       anki  0.008314\n",
       "2447     start  0.007788\n",
       "2727     ttmik  0.007702\n",
       "1296      jlpt  0.006983\n",
       "138        app  0.006177\n",
       "2848  wanikani  0.004682\n",
       "2229       say  0.004388\n",
       "1691      name  0.004383\n",
       "1589   meaning  0.003735\n",
       "974       game  0.003598\n",
       "1552     manga  0.003596\n",
       "2614     thank  0.003488"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_feature_impt(forest_cvec_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "988453a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>kanji</td>\n",
       "      <td>0.046214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>genki</td>\n",
       "      <td>0.016979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>hiragana</td>\n",
       "      <td>0.013641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>japan</td>\n",
       "      <td>0.011720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>n</td>\n",
       "      <td>0.009970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>anki</td>\n",
       "      <td>0.008962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5966</th>\n",
       "      <td>start</td>\n",
       "      <td>0.008296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>korea</td>\n",
       "      <td>0.008064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>katakana</td>\n",
       "      <td>0.006966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>app</td>\n",
       "      <td>0.006885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>jlpt</td>\n",
       "      <td>0.006701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>anime</td>\n",
       "      <td>0.005828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>say</td>\n",
       "      <td>0.005689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851</th>\n",
       "      <td>someone</td>\n",
       "      <td>0.005415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>ttmik</td>\n",
       "      <td>0.005165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>correct</td>\n",
       "      <td>0.004822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6349</th>\n",
       "      <td>thank</td>\n",
       "      <td>0.004819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6876</th>\n",
       "      <td>wanikani</td>\n",
       "      <td>0.004466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>difference</td>\n",
       "      <td>0.004364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4711</th>\n",
       "      <td>please</td>\n",
       "      <td>0.004302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1\n",
       "3361       kanji  0.046214\n",
       "2515       genki  0.016979\n",
       "2825    hiragana  0.013641\n",
       "3245       japan  0.011720\n",
       "4128           n  0.009970\n",
       "298         anki  0.008962\n",
       "5966       start  0.008296\n",
       "3486       korea  0.008064\n",
       "3381    katakana  0.006966\n",
       "340          app  0.006885\n",
       "3291        jlpt  0.006701\n",
       "291        anime  0.005828\n",
       "5464         say  0.005689\n",
       "5851     someone  0.005415\n",
       "6568       ttmik  0.005165\n",
       "1309     correct  0.004822\n",
       "6349       thank  0.004819\n",
       "6876    wanikani  0.004466\n",
       "1615  difference  0.004364\n",
       "4711      please  0.004302"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_feature_impt(forest_tvec_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b782ce2b",
   "metadata": {},
   "source": [
    "For Random Forest Classifier, the important features are similar to the ones picked out by Logistic Regression, but it cannot be determined which subreddit the word is linked to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a81734",
   "metadata": {},
   "source": [
    "## Test Models on New Data\n",
    "To check for future compatibility, one model for each of the 3 algorithms is selected and trained using the original full set of data (from 1 Jan 2021 to 2 Feb 2021) for use on future posts, to ensure that the models are not adversely affected by recent trends. They are tested on 100 newest posts (retrieved on 20 Jan 2022) from each subreddit, and their performances are evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "01200308",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0e2911a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_cvec_tuned_2, logreg_tvec_tuned and forest_cvec_tuned are chosen\n",
    "nb_chosen = set_model('nb', 'cvec', \n",
    "                      max_df=[.2],\n",
    "                      max_features=[5000],\n",
    "                      ngram_range=[(1,3)],\n",
    "                      stopwords=[get_stopwords('common_eng_jk')], \n",
    "                      algo_params={\n",
    "                          'nb__alpha': [.8]\n",
    "                      })\n",
    "\n",
    "logreg_chosen = set_model('logreg', 'tvec', \n",
    "                          max_df=[.2],\n",
    "                          max_features=[5000],\n",
    "                          min_df=[1],\n",
    "                          ngram_range=[(1,1)],\n",
    "                          stopwords=[['japanese', 'korean']], \n",
    "                          algo_params={\n",
    "                              'logreg__C': [.8],\n",
    "                              'logreg__max_iter': [100],\n",
    "                              'logreg__penalty': ['l2']\n",
    "                          })\n",
    "\n",
    "forest_chosen = set_model('forest', 'cvec', \n",
    "                          max_df=[.6],\n",
    "                          max_features=[3000],\n",
    "                          min_df=[1],\n",
    "                          ngram_range=[(1,2)],\n",
    "                          stopwords=[get_stopwords('common_eng_jk')], \n",
    "                          algo_params={\n",
    "                              'forest__n_estimators': [200],\n",
    "                              'forest__max_depth': [None],\n",
    "                              'forest__min_samples_split': [2],\n",
    "                              'forest__ccp_alpha': [0],\n",
    "                              'forest__n_jobs': [-1],\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "982d0b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec',\n",
       "                                        CountVectorizer(token_pattern='\\\\w+')),\n",
       "                                       ('forest',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             param_grid={'cvec__max_df': [0.6], 'cvec__max_features': [3000],\n",
       "                         'cvec__min_df': [1], 'cvec__ngram_range': [(1, 2)],\n",
       "                         'cvec__preprocessor': [None],\n",
       "                         'cvec__stop_words': [['word', 'understand', 'good',\n",
       "                                               'also', 'lot', 'used',\n",
       "                                               'learning', 'much', 'something',\n",
       "                                               'english', 'anyone', 'first',\n",
       "                                               'ive', 'learn', 'know', 'way',\n",
       "                                               'words', 'sentence', 'like',\n",
       "                                               'people', 'help', 'dont', 'get',\n",
       "                                               'grammar', 'really', 'read',\n",
       "                                               'find', 'time', 'want', 'use', ...]],\n",
       "                         'forest__ccp_alpha': [0], 'forest__max_depth': [None],\n",
       "                         'forest__min_samples_split': [2],\n",
       "                         'forest__n_estimators': [200],\n",
       "                         'forest__n_jobs': [-1]})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit on full dataset\n",
    "nb_chosen.fit(X,y)\n",
    "logreg_chosen.fit(X,y)\n",
    "forest_chosen.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33ea42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape the web and put into 'subreddit' and 'post' into DataFrame\n",
    "url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "\n",
    "def get_new_posts(subreddit):\n",
    "    # scrape the web\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100\n",
    "    }\n",
    "    res = requests.get(url, params)\n",
    "    df = pd.DataFrame(res.json()['data'])[['subreddit', 'selftext', 'title']]\n",
    "    \n",
    "    # create feature 'post'\n",
    "    df['post'] = df['title']+' '+df['selftext']\n",
    "    \n",
    "    # remove urls, '[removed]', numbers and punctuations, change to lower case\n",
    "    df['post'] = df['post'].map(lambda x: re.sub(r'\\[removed\\]', '', re.sub(r'http\\S+', '', x)))\n",
    "    df['post'] = df['post'].map(lambda x: re.sub(r'[^\\w\\s]|\\d', '', x))\n",
    "    df['post'] = df['post'].map(lambda x: x.lower())\n",
    "    df['post'] = df['post'].map(lambda x: re.sub(r\"[^\\x00-\\x7F]\", '', x))\n",
    "    \n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(subset='post', inplace=True)\n",
    "\n",
    "    new_df = df[['subreddit','post']]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eece5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new posts\n",
    "j_new = get_new_posts('LearnJapanese')\n",
    "k_new = get_new_posts('korean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58a9698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate sensitivity and specificity, and return dataframe of wrongly classified posts\n",
    "def get_wrong_preds(model):\n",
    "    j_preds = model.predict(j_new['post'])\n",
    "    k_preds = model.predict(k_new['post'])\n",
    "    print(f\"Percentage of Japanese posts predicted correctly: {round(np.mean(j_preds),3)}\")\n",
    "    print(f\"Percentage of Korean posts predicted correctly: {round(1 - np.mean(k_preds),3)}\")\n",
    "    j_wrong = j_new[~np.array(j_preds, dtype=bool)]\n",
    "    k_wrong = k_new[np.array(k_preds, dtype=bool)]\n",
    "    return pd.concat([j_wrong, k_wrong])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aae8630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Japanese posts predicted correctly: 0.794\n",
      "Percentage of Korean posts predicted correctly: 0.796\n"
     ]
    }
   ],
   "source": [
    "# for Multinomial Naive-Bayes\n",
    "wrong_preds_nb = get_wrong_preds(nb_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "204f7a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Japanese posts predicted correctly: 0.691\n",
      "Percentage of Korean posts predicted correctly: 0.786\n"
     ]
    }
   ],
   "source": [
    "# for Logistic Regression\n",
    "wrong_preds_logreg = get_wrong_preds(logreg_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bd26acd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Japanese posts predicted correctly: 0.763\n",
      "Percentage of Korean posts predicted correctly: 0.776\n"
     ]
    }
   ],
   "source": [
    "# for Random Forest Classifier\n",
    "wrong_preds_forest = get_wrong_preds(forest_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e132ca7f",
   "metadata": {},
   "source": [
    "Multinomial Naive-Bayes performs slightly better than Random Forest Classifier, while Logistic Regression performs poorly on sensitivity with only 69% of the Japanese posts correctly classified.\n",
    "\n",
    "The wrongly predicted Japanese posts are investigated and compared below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8f38bf31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Indices of wrong predictions in Japanese subreddit ==============\n",
      "nb: {3, 5, 6, 8, 21, 22, 24, 27, 28, 29, 36, 52, 54, 55, 58, 59, 63, 69, 72, 91}\n",
      "logreg: {2, 5, 8, 19, 21, 22, 24, 27, 28, 33, 36, 37, 38, 39, 52, 54, 55, 58, 59, 61, 62, 63, 68, 69, 72, 75, 83, 84, 86, 91}\n",
      "forest: {2, 4, 5, 16, 19, 21, 24, 27, 28, 33, 36, 38, 54, 55, 58, 61, 62, 63, 64, 69, 71, 83, 84}\n"
     ]
    }
   ],
   "source": [
    "# get indices of wrongly predicted Japanese posts\n",
    "wrong_nb_ind = set(wrong_preds_nb[wrong_preds_nb['subreddit']=='LearnJapanese'].index)\n",
    "wrong_logreg_ind = set(wrong_preds_logreg[wrong_preds_logreg['subreddit']=='LearnJapanese'].index)\n",
    "wrong_forest_ind = set(wrong_preds_forest[wrong_preds_forest['subreddit']=='LearnJapanese'].index)\n",
    "\n",
    "print(\" Indices of wrong predictions in Japanese subreddit \".center(80, '='))\n",
    "print(f\"nb: {wrong_nb_ind}\")\n",
    "print(f\"logreg: {wrong_logreg_ind}\")\n",
    "print(f\"forest: {wrong_forest_ind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7b6bbc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>if im talking to a stranger and they make me mad do i still have to use  form with them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>what does this tattoo mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>classical vs modern japanese i was wondering if anyone knew the difference between modern and classical japanese if you were fluent in modern japanese could you understand classical or are they too different if theyre different how do peoplehistorians learn classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>what are some online chatrooms or discords dedicated to japanese learning ive gone on some of the japanese discords but theyre either  memes and no place to actually ask questions or weird chat setups or only voice channels\\n\\nare there any solid places to talk to communicate with other people that are learning japanese\\n\\notherwise id have to ask my japanese friend about  questions a day can i say  to mean i didnt eat\\n\\nitd get pretty old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>how do i say thank you for your love at me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>how do you say thank you with a name is it thank you akasuki or\\n\\nakasuki thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>can  represent both a change of state and action in progress depending on context i was thinking of how little i understood about the  form of verbs yesterday  specifically i dont understand weather  representing an action in progress or a change of state depends on the verb  for example can  mean either depending on weather your emphasizing the object or the subject  as in\\n\\nampxb\\n\\n\\n\\nim making dinner\\n\\n\\n\\ndinner is made\\n\\nampxb\\n\\nor is this incorrect and  can only take one of the prior two meanings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>is it normal to be able to translate from japanese to english but not english to japanese will say im just started learning again and i am at chaoter  of jfz when ever i have to translate english to japanese i find it really hard but not translating japanese to english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>what does this say   i got this sword that has some japanese letters on it id appreciate if someone could translate this for me \\n\\nthank you so much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>are there any chill discord servers i can join to practice my japanese with other newcomers that isnt competitive about learning title pretty much says it all as a native english speaker slowly learning japanese i just dont know where to look thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>LearnJapanese</td>\n",
       "      <td>is there a phrase in japanese that is equivalent to why not or no reason i got a buzz cut and my coworkers are freaking out well it isnt that bad or mean but many are asking me why i got it \\n\\nis there a way to jokingly respond why not kind of in a shrugging manner or no reason kind of like i just felt like it\\n\\njokingly is kind of important because while they are somwhat used to me being casual the people who keep asking are senpai or just older coworkers they seem to be the most judgmental</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit  \\\n",
       "5   LearnJapanese   \n",
       "21  LearnJapanese   \n",
       "24  LearnJapanese   \n",
       "27  LearnJapanese   \n",
       "28  LearnJapanese   \n",
       "36  LearnJapanese   \n",
       "54  LearnJapanese   \n",
       "55  LearnJapanese   \n",
       "58  LearnJapanese   \n",
       "63  LearnJapanese   \n",
       "69  LearnJapanese   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 post  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                            if im talking to a stranger and they make me mad do i still have to use  form with them   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        what does this tattoo mean   \n",
       "24                                                                                                                                                                                                                                                        classical vs modern japanese i was wondering if anyone knew the difference between modern and classical japanese if you were fluent in modern japanese could you understand classical or are they too different if theyre different how do peoplehistorians learn classical  \n",
       "27                                                                       what are some online chatrooms or discords dedicated to japanese learning ive gone on some of the japanese discords but theyre either  memes and no place to actually ask questions or weird chat setups or only voice channels\\n\\nare there any solid places to talk to communicate with other people that are learning japanese\\n\\notherwise id have to ask my japanese friend about  questions a day can i say  to mean i didnt eat\\n\\nitd get pretty old  \n",
       "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        how do i say thank you for your love at me   \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                               how do you say thank you with a name is it thank you akasuki or\\n\\nakasuki thank you  \n",
       "54  can  represent both a change of state and action in progress depending on context i was thinking of how little i understood about the  form of verbs yesterday  specifically i dont understand weather  representing an action in progress or a change of state depends on the verb  for example can  mean either depending on weather your emphasizing the object or the subject  as in\\n\\nampxb\\n\\n\\n\\nim making dinner\\n\\n\\n\\ndinner is made\\n\\nampxb\\n\\nor is this incorrect and  can only take one of the prior two meanings  \n",
       "55                                                                                                                                                                                                                                                      is it normal to be able to translate from japanese to english but not english to japanese will say im just started learning again and i am at chaoter  of jfz when ever i have to translate english to japanese i find it really hard but not translating japanese to english  \n",
       "58                                                                                                                                                                                                                                                                                                                                                                             what does this say   i got this sword that has some japanese letters on it id appreciate if someone could translate this for me \\n\\nthank you so much   \n",
       "63                                                                                                                                                                                                                                                                       are there any chill discord servers i can join to practice my japanese with other newcomers that isnt competitive about learning title pretty much says it all as a native english speaker slowly learning japanese i just dont know where to look thank you  \n",
       "69                 is there a phrase in japanese that is equivalent to why not or no reason i got a buzz cut and my coworkers are freaking out well it isnt that bad or mean but many are asking me why i got it \\n\\nis there a way to jokingly respond why not kind of in a shrugging manner or no reason kind of like i just felt like it\\n\\njokingly is kind of important because while they are somwhat used to me being casual the people who keep asking are senpai or just older coworkers they seem to be the most judgmental  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display Japanese posts wrongly classified by all 3 models\n",
    "wrong_common = wrong_nb_ind.intersection(wrong_logreg_ind.intersection(wrong_forest_ind))\n",
    "j_new[j_new.index.isin(wrong_common)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066543cf",
   "metadata": {},
   "source": [
    "Reading the actual Japanese posts that are misclassified as Korean, it is not surprising that these posts get misclassified (after the keyword 'japanese' is removed) because the contents are very generic (e.g. 'what does this tattoo mean'). It is unlikely that the models that be improved much further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "68d75fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58742245, 0.77372577, 0.99644074, 0.97754915, 0.78845191,\n",
       "       0.96371974, 0.99999759, 0.94299875, 0.99993798, 0.77869058,\n",
       "       0.99978235])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of intersecting set of wrongly classified Japanese posts being predicted as Korean\n",
    "nb_chosen.predict_proba(j_new[j_new.index.isin(wrong_common)]['post'])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ebbc84d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51831852, 0.69835538, 0.54165618, 0.54345802, 0.69217949,\n",
       "       0.84736174, 0.58750818, 0.64775623, 0.82371365, 0.58802708,\n",
       "       0.55167248])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_chosen.predict_proba(j_new[j_new.index.isin(wrong_common)]['post'])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c7d0683f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63971428, 0.68842689, 0.61691613, 0.56976247, 0.74591667,\n",
       "       0.93916667, 0.51574786, 0.76725   , 0.92416667, 0.775     ,\n",
       "       0.56876374])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_chosen.predict_proba(j_new[j_new.index.isin(wrong_common)]['post'])[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c0634",
   "metadata": {},
   "source": [
    "Multinomial Naive-Bayes does worse than the other 2 models as it assigned very high probabilities of the post being Korean to the common set of misclassified Japanese posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22574b4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "There are sufficient differences between the 2 subreddits as 2 of the 3 trained models (Multinomial Naive-Bayes and Random Forest Classifier) accurately predict which subreddit the posts come from for more than 75% of the time. The Random Forest Classifier is chosen to be the production model as it did not assign very high probabilities to the misclassified posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45783f41",
   "metadata": {},
   "source": [
    "## Possible Improvements in Future\n",
    "1. This project used only 1000 posts from each subreddit. To improve the model, more posts can be scraped from the web so that more data could be used for training.\n",
    "2. More hyperparameter tuning could be attempted to improve the models if there was more time.\n",
    "3. The results of the 3 models could be combined using Voting Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "951cf281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save j_new and k_new to csv for reference\n",
    "j_new.to_csv('datasets/j_new.csv')\n",
    "k_new.to_csv('datasets/k_new.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
